{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii prashant \n"
     ]
    }
   ],
   "source": [
    "print('hii prashant ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/Documents/prashant/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, multiprocessing\n",
    "from datasets import load_dataset\n",
    "# from transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_version, minor_version = torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from flash-attn) (2.3.0+cu118)\n",
      "Requirement already satisfied: einops in /home/vivek/.local/lib/python3.10/site-packages (from flash-attn) (0.8.0)\n",
      "Requirement already satisfied: filelock in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (11.8.86)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from torch->flash-attn) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vivek/Documents/prashant/venv/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=189291963 sha256=b0e1924ee6866db683f5cb72c1002f6d4ca39e9efd79efec1ffc75aaecb2a576\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y2_n7iaq/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.6.3\n",
      "your gpu is noob so its compataible\n"
     ]
    }
   ],
   "source": [
    "if major_version >=8:\n",
    "    !pip install flash-attn\n",
    "    attn_implementation = 'falsh_attentaion_2'\n",
    "    print(\"your gpu is noob so its compataible\")\n",
    "else:\n",
    "    print(\"Your device is noob pro max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/Documents/prashant/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_name  = \"HuggingFaceTB/SmolLM-135M\"\n",
    "from transformers import AutoConfig\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"HuggingFaceTB/SmolLM-135M\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 576,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 9,\n",
       "  \"num_hidden_layers\": 30,\n",
       "  \"num_key_value_heads\": 3,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 49152\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|endoftext|>',\n",
       "  '<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<repo_name>',\n",
       "  '<reponame>',\n",
       "  '<file_sep>',\n",
       "  '<filename>',\n",
       "  '<gh_stars>',\n",
       "  '<issue_start>',\n",
       "  '<issue_comment>',\n",
       "  '<issue_closed>',\n",
       "  '<jupyter_start>',\n",
       "  '<jupyter_text>',\n",
       "  '<jupyter_code>',\n",
       "  '<jupyter_output>',\n",
       "  '<jupyter_script>',\n",
       "  '<empty_output>']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.additional_special_tokens[2]\n",
    "tokenizer.pad_token_id = 2\n",
    "\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train_sft split: 100%|██████████| 207865/207865 [00:05<00:00, 39405.10 examples/s]\n",
      "Generating test_sft split: 100%|██████████| 23110/23110 [00:00<00:00, 39716.04 examples/s]\n",
      "Generating train_gen split: 100%|██████████| 256032/256032 [00:05<00:00, 50537.60 examples/s]\n",
      "Generating test_gen split: 100%|██████████| 28304/28304 [00:00<00:00, 50364.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train_sft = load_dataset('HuggingFaceH4/ultrachat_200k', split='train_sft')\n",
    "dataset_test_sft = load_dataset('HuggingFaceH4/ultrachat_200k', split='test_sft')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\",\n",
       " 'prompt_id': 'f0e37e9f7800261167ce91143f98f511f768847236f133f2d0aed60b444ebe57',\n",
       " 'messages': [{'content': \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you provide me with a link to the documentation for my theme?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you confirm if this feature also works for the Quick Shop section of my theme?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_sft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'How does the author propose to fix the problem of science alienation in our educational system? What changes does she suggest should be made to science education? Answer according to: Science education should be split into two tracks.\\nSplit K-12 science education into two tracks, for majors and nonmajors.\\nThose who want to specialize in science could take math and complex chemistry. Nonmajors would focus on science of the everyday—things like kitchen chemistry and CSI-style crime investigations.\\nSome years ago, when I was working as a newspaper science writer in California, I fell into a rather idle conversation with a physicist on the subject of science education. Idle for him, at least, because what he said—the way he defined the American system of teaching K-12 students about science—has stayed with me since.\\nThis conversation has returned to me many times over the years. On the day my older son, who spent his childhood joyfully chasing insects and reading natural history books, came home from high school to announce that “biology is the most boring subject in the world.” The time the student adviser at the University of Wisconsin journalism school, where I teach, told me that undergrads don’t want to take science-writing classes because they already felt “beaten up by science.” Even on days when I read science literacy statistics and am startled by the percentage of people who think that antibiotics kill viruses as well as bacteria or that the sun revolves around the Earth.\\nIt’s easy for me to join with the chorus of concern over whether we are keeping up as a science-literate country. But even so, science literacy is the wrong term for the worried point I’m making here. I’m concerned about science alienation. The way we teach K-12 science, with its emphasis on the intricacies of formula and mathematical structure necessary for the next generation of scientists and engineers, shuts too many people out. I don’t mean just shut out of the priesthood here; there’s no reason for everyone to become a scientist. But shut out of the comfort zone, of the ability to think of science as something useful in daily life.\\nAnd it’s the science-alienated who matter to me. Partly because as a science writer I find them the most interesting audience—I love the idea of seducing someone convinced that science is boring or meaningless into reading a research-centered story. Partly because I worry that what The New Yorker’s Michael Specter describes as a culture of science denialism is having a profound—possibly dangerous—effect on public policies. Think, for instance, of recent moves in Virginia, Texas, and North Carolina to deny that sea level rise is related to global climate change. And partly I just think that the filtered-out are cheated. They endure the mandatory classes, but they never really learn the main lesson—the way that science helps us understand ourselves and the world around us.\\nThe great psychologist and philosopher William James wrote more than 100 years ago that the science establishment would face consequences if it dismissed the importance of the lay public. In the year 1909, he said, it was easy to find people who no longer paid attention to the teachings of researchers. “They are indifferent to science,” he explained, “because science is so callously indifferent to their experiences.” Today, this is very relevant to K-12 science education: Our system breeds that indifference.\\nHow can we fix it? By requiring even more science education—but changing it dramatically. All high school students should have to take four years of science, instead of the minimal two usually required for graduation. Along with that, we should also offer separate “science tracks”—something like the old tracks for vocational or college-bound students. Those who want to specialize in science could take math, physics, and complex chemistry (and the kind of genetically detailed biology that my son disliked). Nonmajors, by contrast, would focus on what I think of as science of the everyday—things like kitchen chemistry, CSI-style crime investigation, the biology of health, and a class in the physics of sports that would include playing the games.\\nMy tracking system could easily build on existing science electives. My son’s school, for instance, teaches a forensic science unit which uses the classroom as an ever changing “crime scene,” exploring everything from DNA analysis to blood typing. It’s a way of putting science in context. More students would take such classes; more would be exposed to these ideas as they did the coursework needed to complete each track. Ideally, the tracks could be coordinated with the science classes offered for majors and nonmajors at the college level. Of course, occasionally a student might want to switch tracks—and we should allow for that, too. It could go both ways: Would-be scientists might realize that the field isn’t really for them and switch to the nonmajor track, while some students might be inspired by the CSI class to move into the more difficult sciences.\\nWe need to stop trying to teach every child as either a future scientist or a future failed scientist. We don’t want or need every student to be a scientist, but we do want each one to be a success. And success includes a solid understanding and appreciation of science, one that will remain useful to both themselves and society throughout their lives.\\nAlso in Slate’s special issue on science education: Fred Kaplan explains why another “Sputnik moment” would be impossible; Philip Plait explains why he became the “Bad Astronomer”; Paul Plotz describes how almost blowing up his parents’ basement made him a scientist; Tom Kalil says that the Obama administration is using the Make movement to encourage science education; and Dana Goldstein explains why you should make your daughter play video games. Also, share your ideas for fixing science education in the Hive. This article arises from Future Tense, a joint partnership of Slate, the New America Foundation, and Arizona State University.',\n",
       " 'prompt_id': '9fb649a870769f4881c647d20d178656f67fc881b2dc0b65d4860237c2c8da6c',\n",
       " 'messages': [{'content': 'How does the author propose to fix the problem of science alienation in our educational system? What changes does she suggest should be made to science education? Answer according to: Science education should be split into two tracks.\\nSplit K-12 science education into two tracks, for majors and nonmajors.\\nThose who want to specialize in science could take math and complex chemistry. Nonmajors would focus on science of the everyday—things like kitchen chemistry and CSI-style crime investigations.\\nSome years ago, when I was working as a newspaper science writer in California, I fell into a rather idle conversation with a physicist on the subject of science education. Idle for him, at least, because what he said—the way he defined the American system of teaching K-12 students about science—has stayed with me since.\\nThis conversation has returned to me many times over the years. On the day my older son, who spent his childhood joyfully chasing insects and reading natural history books, came home from high school to announce that “biology is the most boring subject in the world.” The time the student adviser at the University of Wisconsin journalism school, where I teach, told me that undergrads don’t want to take science-writing classes because they already felt “beaten up by science.” Even on days when I read science literacy statistics and am startled by the percentage of people who think that antibiotics kill viruses as well as bacteria or that the sun revolves around the Earth.\\nIt’s easy for me to join with the chorus of concern over whether we are keeping up as a science-literate country. But even so, science literacy is the wrong term for the worried point I’m making here. I’m concerned about science alienation. The way we teach K-12 science, with its emphasis on the intricacies of formula and mathematical structure necessary for the next generation of scientists and engineers, shuts too many people out. I don’t mean just shut out of the priesthood here; there’s no reason for everyone to become a scientist. But shut out of the comfort zone, of the ability to think of science as something useful in daily life.\\nAnd it’s the science-alienated who matter to me. Partly because as a science writer I find them the most interesting audience—I love the idea of seducing someone convinced that science is boring or meaningless into reading a research-centered story. Partly because I worry that what The New Yorker’s Michael Specter describes as a culture of science denialism is having a profound—possibly dangerous—effect on public policies. Think, for instance, of recent moves in Virginia, Texas, and North Carolina to deny that sea level rise is related to global climate change. And partly I just think that the filtered-out are cheated. They endure the mandatory classes, but they never really learn the main lesson—the way that science helps us understand ourselves and the world around us.\\nThe great psychologist and philosopher William James wrote more than 100 years ago that the science establishment would face consequences if it dismissed the importance of the lay public. In the year 1909, he said, it was easy to find people who no longer paid attention to the teachings of researchers. “They are indifferent to science,” he explained, “because science is so callously indifferent to their experiences.” Today, this is very relevant to K-12 science education: Our system breeds that indifference.\\nHow can we fix it? By requiring even more science education—but changing it dramatically. All high school students should have to take four years of science, instead of the minimal two usually required for graduation. Along with that, we should also offer separate “science tracks”—something like the old tracks for vocational or college-bound students. Those who want to specialize in science could take math, physics, and complex chemistry (and the kind of genetically detailed biology that my son disliked). Nonmajors, by contrast, would focus on what I think of as science of the everyday—things like kitchen chemistry, CSI-style crime investigation, the biology of health, and a class in the physics of sports that would include playing the games.\\nMy tracking system could easily build on existing science electives. My son’s school, for instance, teaches a forensic science unit which uses the classroom as an ever changing “crime scene,” exploring everything from DNA analysis to blood typing. It’s a way of putting science in context. More students would take such classes; more would be exposed to these ideas as they did the coursework needed to complete each track. Ideally, the tracks could be coordinated with the science classes offered for majors and nonmajors at the college level. Of course, occasionally a student might want to switch tracks—and we should allow for that, too. It could go both ways: Would-be scientists might realize that the field isn’t really for them and switch to the nonmajor track, while some students might be inspired by the CSI class to move into the more difficult sciences.\\nWe need to stop trying to teach every child as either a future scientist or a future failed scientist. We don’t want or need every student to be a scientist, but we do want each one to be a success. And success includes a solid understanding and appreciation of science, one that will remain useful to both themselves and society throughout their lives.\\nAlso in Slate’s special issue on science education: Fred Kaplan explains why another “Sputnik moment” would be impossible; Philip Plait explains why he became the “Bad Astronomer”; Paul Plotz describes how almost blowing up his parents’ basement made him a scientist; Tom Kalil says that the Obama administration is using the Make movement to encourage science education; and Dana Goldstein explains why you should make your daughter play video games. Also, share your ideas for fixing science education in the Hive. This article arises from Future Tense, a joint partnership of Slate, the New America Foundation, and Arizona State University.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The author proposes to fix the problem of science alienation in our educational system by splitting K-12 science education into two tracks, for majors and nonmajors. Those who want to specialize in science could take math and complex chemistry, while nonmajors would focus on science of the everyday – things like kitchen chemistry and CSI-style crime investigations. All high school students should take four years of science, and the tracking system could build on existing science electives. The goal is to teach every student a solid understanding and appreciation of science, one that will remain useful to both themselves and society throughout their lives.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you provide examples of how the proposed tracking system for science education could be implemented in schools?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'There are several ways in which the proposed tracking system for science education could be implemented in schools. Here are a few examples:\\n\\n1. One approach could be to offer different science courses for majors and nonmajors starting in middle school. For example, in 6th grade, students could be divided into two tracks, and each track would take a different science course. In the major track, students would take a course that focuses on the foundations of science, including an emphasis on mathematics and physics. In the nonmajor track, students would take a course that focuses on science concepts that are relevant to everyday life, such as health and nutrition.\\n\\n2. Another approach could be to offer more advanced science courses in high school for students who wish to pursue a career in science. This could include courses in advanced physics, chemistry, and biology. For students who are not interested in a science career, science courses could be focused on more practical applications of science, such as environmental science or food science.\\n\\n3. Schools could also offer a wider range of science electives to cater to the varied interests of students. For example, students interested in engineering could take courses in robotics or renewable energy, while those interested in the medical field could take courses in anatomy and physiology.\\n\\n4. Hands-on science experiments and projects could be incorporated into science classes to make learning more engaging and relevant. For example, students could conduct experiments in their own kitchens to learn about chemical reactions, or participate in a mock crime scene investigation to learn about forensic science.\\n\\nOverall, the tracking system for science education proposed by the author could be implemented in a variety of ways, depending on the specific needs and interests of students in each school.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you explain how the proposed tracking system for science education would benefit students who are not interested in pursuing a career in science?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The proposed tracking system for science education would benefit students who are not interested in pursuing a career in science in several ways. First, it would help them develop an understanding and appreciation of science concepts that are relevant to everyday life, such as health, nutrition, and the environment. This would help them make informed decisions in their personal lives and understand how science impacts society as a whole.\\n\\nSecond, focusing on science concepts that are practical and engaging would make science more interesting and relevant to non-science students. This could help them overcome their science alienation and develop a sense of curiosity and wonder about science.\\n\\nLastly, the proposed tracking system would offer non-science students a wider range of science courses and electives to choose from, which would allow them to explore different areas of science according to their interests. For example, a student who is interested in sports could take a physics of sports class, while a student who is interested in cooking could take a kitchen chemistry class.\\n\\nOverall, the proposed tracking system for science education would benefit non-science students by providing them with a solid understanding and appreciation of science concepts that are relevant to their everyday lives, increasing their interest in science, and offering them more opportunities to explore science according to their interests.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you elaborate on how offering separate science tracks would inspire students to explore science further and possibly change their career path?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Offering separate science tracks would inspire students to explore science further and possibly change their career path by exposing them to a wider range of science concepts that are relevant to their interests and providing them with the opportunity to engage in hands-on, practical science activities that they may enjoy.\\n\\nFor example, a student who takes a kitchen chemistry class in the non-major track may discover a newfound interest in food science and decide to pursue a career in that field. Similarly, a student who takes a physics of sports class may develop an interest in sports science and decide to pursue that field.\\n\\nFurthermore, by offering science courses that are more practical and engaging, students who may have previously been disinterested in science may discover a passion for a particular subject or field within science. This could motivate them to pursue further studies in that area and even consider a career in science.\\n\\nOverall, by offering separate science tracks, schools would be creating opportunities for students to explore science according to their interests and discover new areas of science that they may be passionate about. This could inspire them to change their career path and pursue a career in science, even if they had not previously considered it. The more science-literate our society is, the more equipped we are to tackle challenges and advance scientific discoveries.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_sft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# if major_version >= 8:\n",
    "#   !pip install flash-attn\n",
    "#   attn_implementation='flash_attention_2'\n",
    "#   print(\"Your GPU is compatible with FlashAttention.\")\n",
    "# else:\n",
    "#   attn_implementation='eager'\n",
    "#   print(\"Your GPU is not compatible with FlashAttention.\")\n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = \"<|im_end|>\"\n",
    "tokenizer.pad_token_id = 2\n",
    "tokenizer.padding_side = 'left' #Necessary for FlashAttention compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, attn_implementation=\"eager\", device_map={\"\": 0}\n",
    ")\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_sft = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
    "dataset_test_sft = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft[:5%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\",\n",
       " 'prompt_id': 'f0e37e9f7800261167ce91143f98f511f768847236f133f2d0aed60b444ebe57',\n",
       " 'messages': [{'content': \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you provide me with a link to the documentation for my theme?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Can you confirm if this feature also works for the Quick Shop section of my theme?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_sft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 207865/207865 [00:42<00:00, 4945.36 examples/s]\n",
      "Map: 100%|██████████| 1156/1156 [00:00<00:00, 4910.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(example):\n",
    "    \"\"\"Format the dataset into the expected conversation format\"\"\"\n",
    "    # The messages are already in the correct format, we just need to normalize the roles\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": msg[\"role\"].replace(\"human\", \"user\"),  # Convert 'human' to 'user' if needed\n",
    "            \"content\": msg[\"content\"]\n",
    "        }\n",
    "        for msg in example[\"messages\"]\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "dataset_train_sft = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
    "dataset_test_sft = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft[:5%]\")\n",
    "\n",
    "dataset_train_sft = dataset_train_sft.map(\n",
    "    format_dataset,\n",
    "    remove_columns=dataset_train_sft.column_names\n",
    ")\n",
    "dataset_test_sft = dataset_test_sft.map(\n",
    "    format_dataset,\n",
    "    remove_columns=dataset_test_sft.column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, attn_implementation=\"eager\", device_map={\"\": 0}\n",
    ")\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "        output_dir=\"./sft_smollm_135M/\",\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=500,\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "        fp16= not torch.cuda.is_bf16_supported(),\n",
    "        bf16= torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=50,\n",
    "        max_steps=4000,\n",
    "        warmup_steps=30,\n",
    "        max_seq_length=2048,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/builder.py:1607\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1606\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, record \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/packaged_modules/generator/generator.py:33\u001b[0m, in \u001b[0;36mGenerator._generate_examples\u001b[0;34m(self, **gen_kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, ex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgenerator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m idx, ex\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:586\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_packed_dataloader.<locals>.data_generator\u001b[0;34m(constant_length_iterator)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_generator\u001b[39m(constant_length_iterator):\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m constant_length_iterator\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/trl/trainer/utils.py:643\u001b[0m, in \u001b[0;36mConstantLengthDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 643\u001b[0m     buffer\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatting_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    644\u001b[0m     buffer_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buffer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/trl/extras/dataset_formatting.py:43\u001b[0m, in \u001b[0;36mconversations_formatting_function.<locals>.format_dataset\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages_field\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1803\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1967\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1972\u001b[0m         )\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:589\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_packed_dataloader\u001b[0;34m(self, processing_class, dataset, dataset_text_field, max_seq_length, num_of_sequences, chars_per_token, formatting_func, append_concat_token, add_special_tokens)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     packed_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconstant_length_iterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant_length_iterator\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (DatasetGenerationError, SchemaInferenceError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:1099\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, split, **kwargs)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneratorDatasetInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/io/generator.py:49\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mas_dataset(\n\u001b[1;32m     57\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msplit, verification_mode\u001b[38;5;241m=\u001b[39mverification_mode, in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_in_memory\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/builder.py:1648\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/builder.py:1486\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1486\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1487\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1488\u001b[0m     ):\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/datasets/builder.py:1643\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1642\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_train_sft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_test_sft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:368\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m         _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:488\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, processing_class, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_non_packed_dataloader(\n\u001b[1;32m    478\u001b[0m         processing_class,\n\u001b[1;32m    479\u001b[0m         dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m         remove_unused_columns,\n\u001b[1;32m    485\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_packed_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappend_concat_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/prashant/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:593\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_packed_dataloader\u001b[0;34m(self, processing_class, dataset, dataset_text_field, max_seq_length, num_of_sequences, chars_per_token, formatting_func, append_concat_token, add_special_tokens)\u001b[0m\n\u001b[1;32m    589\u001b[0m     packed_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[1;32m    590\u001b[0m         data_generator, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant_length_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m: constant_length_iterator}\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (DatasetGenerationError, SchemaInferenceError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while packing the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that your dataset has enough samples to at least yield one packed sequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packed_dataset\n",
      "\u001b[0;31mValueError\u001b[0m: Error occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence."
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_train_sft,\n",
    "        eval_dataset=dataset_test_sft,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First example: {'prompt': \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\", 'prompt_id': 'f0e37e9f7800261167ce91143f98f511f768847236f133f2d0aed60b444ebe57', 'messages': [{'content': \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\", 'role': 'user'}, {'content': 'This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.', 'role': 'assistant'}, {'content': 'Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?', 'role': 'user'}, {'content': \"Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\", 'role': 'assistant'}, {'content': 'Can you provide me with a link to the documentation for my theme?', 'role': 'user'}, {'content': \"I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\", 'role': 'assistant'}, {'content': 'Can you confirm if this feature also works for the Quick Shop section of my theme?', 'role': 'user'}, {'content': \"The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst example:\", dataset_train_sft[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = \"<|im_end|>\"\n",
    "tokenizer.pad_token_id = 2\n",
    "tokenizer.padding_side = 'left' #Necessary for FlashAttention compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "Human: {human_input}\n",
    "Assistant: {assistant_output}\n",
    "\"\"\"\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_sft = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
    "dataset_test_sft = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft[:5%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = \"<|im_end|>\"\n",
    "tokenizer.pad_token_id = 2\n",
    "tokenizer.padding_side = 'left' #Necessary for FlashAttention compatibility\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "        output_dir=\"./sft_smollm_135M/\",\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=500,\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "        fp16= not torch.cuda.is_bf16_supported(),\n",
    "        bf16= torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=50,\n",
    "        max_steps=4000,\n",
    "        warmup_steps=30,\n",
    "        max_seq_length=2048,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m,\n\u001b[1;32m      3\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m dataset_train_sft,\n\u001b[1;32m      4\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m dataset_test_sft,\n\u001b[1;32m      5\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m      6\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_arguments,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset_train_sft,\n",
    "    eval_dataset = dataset_test_sft,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaukllp][bbcvdec dnn rtyyuuip[[8+++-\\    n ]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
